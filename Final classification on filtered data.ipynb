{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "\n",
    "Please note that before running the code, you will need all the GDSC files. They can be found on the drive (https://drive.google.com/drive/u/1/folders/11omvpOttkdZZgv_ppbtcCbojkuVR-D61) or directly on the GDSC website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE = \"AUC\" #how the sensitivity of the drug is computed; other option: \"AUC\" \n",
    "SAVE = False #whether to save the final matrix or not\n",
    "PATH = \"data/Final matrices/\" #where your data is located\n",
    "\n",
    "N_SPLITS = 5\n",
    "DRUG_NAMES = {\"CI-1040\": [\"MYC_mut\",\"RB1_mut\",\"ERBB2_amp\",\"BRAF_mut\",\"KRAS_mut\",\"NRAS_mut\"],\n",
    "              \"PD0325901\": [\"MYC_mut\",\"RB1_mut\",\"ERBB2_amp\",\"BRAF_mut\",\"KRAS_mut\",\"NRAS_mut\"],\n",
    "              \"Refametinib\":[\"MYC_mut\",\"RB1_mut\",\"ERBB2_amp\",\"BRAF_mut\",\"KRAS_mut\",\"NRAS_mut\"],\n",
    "              \"VX-11e\":[\"RB1_mut\",\"ERBB2_amp\",\"CCND1_amp\",\"BRAF_mut\",\"KRAS_mut\",\"NRAS_mut\"],\n",
    "              \"Afatinib\":[\"KRAS_mut\",\"NRAS_mut\",\"EGFR_amp\",\"ERBB2_amp\",\"FOXP3_del\"],\n",
    "              \"Pelitinib\":[\"BRAF_mut\",\"RB1_mut\",\"MAPK1_del\",\"MYC_mut\",\"EGFR_mut\",\"CDKN1B_del\"]\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \n",
    "    feature_names = list(X_train) # Store the feature names\n",
    "    \n",
    "    sc = StandardScaler()  # Defines the scaler\n",
    "    X_train = pd.DataFrame(sc.fit_transform(X_train))  # Scales the training data\n",
    "    X_test = pd.DataFrame(sc.transform(X_test))  # Scales the validation data\n",
    "\n",
    "    # Replace feature names in the database (they are lost during scaling)\n",
    "    X_train.columns = feature_names\n",
    "    X_test.columns = feature_names\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_feature_reduction(X_train, X_val, y_train, Cst = 0.01):\n",
    "    \n",
    "    clf = LinearSVC(C = Cst, penalty = \"l1\", dual = False) #SVC(kernel = 'linear', C = Cst)\n",
    "    clf.fit(X_train.values, y_train)\n",
    "    \n",
    "    model = SelectFromModel(clf, prefit=True, threshold=-np.inf, max_features = 300)\n",
    "    new_X_train = model.transform(X_train.values)\n",
    "    new_X_val = model.transform(X_val.values)\n",
    "    \n",
    "    return pd.DataFrame(new_X_train), pd.DataFrame(new_X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_feature_reduction(X_train, X_val, y_train, N_ESTIMATORS = 500):\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = N_ESTIMATORS, n_jobs = -1, class_weight = \"balanced\", random_state = 32)\n",
    "    clf = clf.fit(X_train.values, y_train.values)\n",
    "    \n",
    "    model = SelectFromModel(clf, prefit=True, threshold=-np.inf, max_features = 300)   \n",
    "    new_X_train = model.transform(X_train)\n",
    "    new_X_val = model.transform(X_val)\n",
    "    \n",
    "    importances = clf.feature_importances_\n",
    "    #std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0) --> std dev but we don't use it anyway\n",
    "    \n",
    "    return pd.DataFrame(new_X_train), pd.DataFrame(new_X_val), importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_PCA(X, NB_COMPONENTS, kf = 5, verbose = 5):\n",
    "\n",
    "    if verbose > 1:\n",
    "        print(\"Running PCA...\")\n",
    "        print(\"Number of features before PCA: \", len(X.columns))\n",
    "\n",
    "    # Starting PCA\n",
    "    my_PCA = PCA(n_components=NB_COMPONENTS)\n",
    "    reduced_X = pd.DataFrame(my_PCA.fit_transform(np.array(X.values), y=None))\n",
    "    reduced_X.columns = [f\"PC{elem}\" for elem in range(NB_COMPONENTS)]\n",
    "    \n",
    "    # Plotting\n",
    "    if verbose > 0:\n",
    "        fig = plt.figure()\n",
    "        g = sns.lineplot(x = range(1, NB_COMPONENTS+1), y = my_PCA.explained_variance_ratio_)\n",
    "        plt.xlabel('Principal Component')\n",
    "        plt.ylabel('Variance (Component Scores)')\n",
    "        plt.title('Screen Plot of Principal Components')\n",
    "        plt.show(); \n",
    "    \n",
    "    if verbose > 1:\n",
    "        print(\"Number of combined features after PCA: \",len(list(reduced_X)))\n",
    "    \n",
    "    return reduced_X   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performances(DRUG_NAME, ML_matrix, PATH, SCORE, N_SPLITS, verbose = 5):\n",
    "    \n",
    "    ## Look at the distribution of the response\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(f\"\\nDistribution of responses for {DRUG_NAME}\")\n",
    "        fig = plt.figure(figsize = (10,6))\n",
    "        sns.distplot(ML_matrix[[DRUG_NAME]])\n",
    "        plt.title(\"Plot of the distribution of AUC values for \"+ DRUG_NAME)\n",
    "        plt.xlabel(\"AUC\")\n",
    "        plt.ylabel(\"Proportion of cell poplations\")\n",
    "        plt.grid(True);\n",
    "        plt.show()\n",
    "\n",
    "    #_______________________________________________________________________________________________________#\n",
    "    \n",
    "    ## Data Categorisation and feature matrix preparation\n",
    "    print(\"\\nCategorisation of the data...\")\n",
    "    # The data is divided in quartiles. Upper/lower third quartile thresholds were used for discretization. \n",
    "    # Everything that is in the middle is discarded. See [this](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3995541/) paper.\n",
    "    # Splitting at the median or mean, although keeping more cells, leads to poorer results.\n",
    "    \n",
    "    if verbose > 1:\n",
    "        print(\"The lower threshold used here is the lower third quartile = \", ML_matrix[DRUG_NAME].quantile([0.25,0.75])[0.25])\n",
    "        print(\"The upper threshold used here is the uppder third quartile = \", ML_matrix[DRUG_NAME].quantile([0.25,0.75])[0.75])\n",
    "    ML_matrix[\"Response\"] = pd.cut(ML_matrix[DRUG_NAME], [np.min(ML_matrix[DRUG_NAME]), ML_matrix[DRUG_NAME].quantile([0.25,0.75])[0.25], ML_matrix[DRUG_NAME].quantile([0.25,0.75])[0.75], np.max(ML_matrix[DRUG_NAME])], labels = [\"sensitive\",\"medium\", \"resistant\"])\n",
    "    ML_matrix = ML_matrix.drop([DRUG_NAME], axis = 1)\n",
    "    \n",
    "    #Drop all the \"medium\" classes and NaNs\n",
    "    ML_matrix = ML_matrix.drop(ML_matrix[ML_matrix[\"Response\"] == \"medium\"].index)\n",
    "    ML_matrix = ML_matrix.dropna()  \n",
    "    \n",
    "    # Convert the response to a float\n",
    "    ML_matrix[\"Response\"] = [0 if x=='resistant' else 1 for x in ML_matrix['Response']]\n",
    "    \n",
    "    # Structure of the feature matrix\n",
    "    print(f\"\\tStructure of the final matrix for {DRUG_NAME}:\")\n",
    "    print(\"\\t\\tNumber of resistant cells: \", len(ML_matrix[ML_matrix[\"Response\"] == 0]))\n",
    "    print(\"\\t\\tNumber of sensitive cells: \", len(ML_matrix[ML_matrix[\"Response\"] == 1]))\n",
    "    print(\"\\t\\tTotal number of cells: \", len(ML_matrix.index))\n",
    "    print(f'\\n\\t\\tThe baseline accuracy for {DRUG_NAME} is {100*np.round(len(ML_matrix[ML_matrix[\"Response\"] == 1])/len(ML_matrix.index),2)}%')\n",
    "    \n",
    "    #_______________________________________________________________________________________________________#\n",
    "    \n",
    "    ## Run the different models and feature selection methods\n",
    "    \n",
    "    print(f\"\\nRunning the models with a {N_SPLITS}-fold cross-validation...\")\n",
    "    print(\"Please wait. This can take up to a minute.\")\n",
    "    \n",
    "    # Create our feature and response matrix\n",
    "    X = ML_matrix.drop(\"Response\", axis = 1)\n",
    "    y = ML_matrix[\"Response\"].astype('float64')\n",
    "        \n",
    "    # Define our models    \n",
    "    models = [SVC(random_state=40),\n",
    "              LogisticRegression(solver= 'saga', max_iter=5000, random_state=41),\n",
    "              MLPClassifier(max_iter=5000, random_state=42),\n",
    "              RandomForestClassifier(n_estimators=300, max_depth = 3, max_leaf_nodes = 10, random_state=43)]\n",
    "    \n",
    "    model_names = ['SVM', 'Logistic Regression', 'MLP', 'Random Forest']\n",
    "    \n",
    "    # Define our final result matrix\n",
    "    final_results = pd.DataFrame(columns = model_names)     \n",
    "          \n",
    "    # Run all models\n",
    "    # Run PCA\n",
    "    final_results = pretty_run(final_results, X, y, models, model_names, N_SPLITS, DRUG_NAME, FS = \"PCA\", index_name = \"Principal Component Analysis\", verbose = verbose)  \n",
    "          \n",
    "    # Run RFE\n",
    "    final_results = pretty_run(final_results, X, y, models, model_names, N_SPLITS, DRUG_NAME, FS = \"RFE\", index_name = \"Recursive Feature Elimination\", verbose = verbose)\n",
    "    \n",
    "    # Run Lasso Feature Selection\n",
    "    final_results = pretty_run(final_results, X, y, models, model_names, N_SPLITS, DRUG_NAME, FS = \"LASSO\", index_name = \"Lasso Feature Selection\", verbose = verbose)\n",
    "          \n",
    "    # Run Random Forest Feature Selection\n",
    "    final_results = pretty_run(final_results, X, y, models, model_names, N_SPLITS, DRUG_NAME, FS = \"RF\", index_name = \"Random Forest Feature Selection\", verbose = verbose)\n",
    "          \n",
    "    # Run with no feature selection\n",
    "    final_results = pretty_run(final_results, X, y, models, model_names, N_SPLITS, DRUG_NAME, FS = \"None\", index_name = \"No feature selection\", verbose = verbose)\n",
    "    \n",
    "    print(f\"All the models were run on {DRUG_NAME} successfully!\")\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_run(final_results, X, y, models, model_names, N_SPLITS, DRUG_NAME, FS, index_name, verbose = 5):\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        if verbose > 0:\n",
    "            print('______________________\\n')\n",
    "            print('Running',model_names[i],'and',index_name)\n",
    "        perf = run_model(X, y, models[i], FS = FS, n_splits = N_SPLITS, verbose = verbose)\n",
    "        final_results.loc[index_name,model_names[i]] = perf[2]\n",
    "    \n",
    "    final_results.loc[index_name,\"Drug\"] = DRUG_NAME\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X, y, clf, FS = \"None\", n_splits = 5, verbose = 5):\n",
    "    \n",
    "    train_accuracies, val_accuracies, rocs = [],[],[]\n",
    "    \n",
    "    if FS == \"PCA\":\n",
    "        X = run_PCA(X, NB_COMPONENTS = len(X.columns), verbose = verbose)\n",
    "        \n",
    "    # Define a cross-validation (shuffleSplit here)\n",
    "    ss = ShuffleSplit(n_splits, test_size=0.2, random_state=0)\n",
    "\n",
    "    for count, (training_indices, val_indices) in enumerate(ss.split(X, y), 1):\n",
    "        \n",
    "        if verbose > 1:\n",
    "            print(f'Cross-validation: {count}/{n_splits}')\n",
    "\n",
    "        # Prepare the test and training set     \n",
    "        X_train = X.iloc[training_indices,:]\n",
    "        y_train = y.iloc[training_indices]\n",
    "        X_val = X.iloc[val_indices,:]\n",
    "        y_val = y.iloc[val_indices]\n",
    "        if verbose > 1:\n",
    "            print(f\"The validation set corresponds to roughly {np.round(100*(len(X_val.index)/len(X.index)),2)}% of the total data.\") \n",
    "        \n",
    "        #Scaling the features -- useful for most classifier except RF and co\n",
    "        X_train, X_val = scale_data(X_train, X_val)\n",
    "        \n",
    "        if FS == \"RFE\":            \n",
    "            selector = RFE(estimator = LinearSVC())\n",
    "            selector = selector.fit(X_train, y_train)\n",
    "            X_train = X_train[X_train.columns[selector.support_]]\n",
    "            X_val = X_val[X_val.columns[selector.support_]]\n",
    "        elif FS == \"LASSO\":\n",
    "            X_train, X_val = lasso_feature_reduction(X_train, X_val, y_train)\n",
    "        elif FS == \"RF\":\n",
    "            X_train, X_val, impor = rf_feature_reduction(X_train, X_val, y_train)\n",
    "        elif (FS == \"None\") | (FS == \"PCA\"):\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Check your feature selection method. Enter either 'RFE', LASSO', 'RF' or 'None'.\")\n",
    "            return\n",
    "        \n",
    "        # Fit the classifier\n",
    "        clf.fit(X_train.values.tolist(), y_train.values)        \n",
    "        \n",
    "        # Predict the classes\n",
    "        y_pred = clf.predict(X_val.values.tolist())\n",
    "        \n",
    "        # Calculate the performance metrics \n",
    "        train_acc = accuracy_score(y_train.values.tolist(), clf.predict(X_train.values.tolist()))\n",
    "        val_acc = accuracy_score(y_val.values.tolist(), y_pred)\n",
    "        #roc_auc = roc_auc_score(y_val.values, clf.predict_proba(X_val.values)[:, 1])\n",
    "        \n",
    "        if verbose > 1:\n",
    "            print(f\"Training accuracy {count}: {train_acc}\")\n",
    "            print(f\"Validation accuracy {count}: {val_acc}\")\n",
    "            #print(f\"ROC AUC {count}: {roc_auc}\")\n",
    "    \n",
    "        # Add the performances to their corresponding lists\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        #rocs.append(roc_auc)\n",
    "    if verbose > 1:\n",
    "        print(\"_________________________________________________________________________________\")   \n",
    "    if verbose > 0:\n",
    "        print(f'\\tAverage Training Accuracy: {np.round(100*np.mean(train_accuracies), 2)} +/- {np.round(100*np.std(train_accuracies),2)}%.')\n",
    "        print(f'\\tAverage Validation Accuracy: {np.round(100*np.mean(val_accuracies),2)} +/- {np.round(100*np.std(val_accuracies),2)}%.')\n",
    "        #print(f'Average AUC {np.round(100*np.mean(np.array(rocs)),2)} +/- {np.round(100*np.std(np.array(rocs)),2)}%.')\n",
    "    \n",
    "    return 100*np.mean(train_accuracies), 100*np.std(train_accuracies), 100*np.mean(val_accuracies), 100*np.std(val_accuracies)#, 100*np.mean(np.array(rocs)), 100*np.std(np.array(rocs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigating CI-1040... \n",
      "Number of cells:  758\n",
      "\n",
      "Categorisation of the data...\n",
      "\tStructure of the final matrix for CI-1040:\n",
      "\t\tNumber of resistant cells:  190\n",
      "\t\tNumber of sensitive cells:  189\n",
      "\t\tTotal number of cells:  379\n",
      "\n",
      "\t\tThe baseline accuracy for CI-1040 is 50.0%\n",
      "\n",
      "Running the models with a 5-fold cross-validation...\n",
      "Please wait. This can take up to a minute.\n",
      "All the models were run on CI-1040 successfully!\n",
      "_________________________________________________________________________________________________________\n",
      "Investigating PD0325901... \n",
      "Number of cells:  759\n",
      "\n",
      "Categorisation of the data...\n",
      "\tStructure of the final matrix for PD0325901:\n",
      "\t\tNumber of resistant cells:  190\n",
      "\t\tNumber of sensitive cells:  189\n",
      "\t\tTotal number of cells:  379\n",
      "\n",
      "\t\tThe baseline accuracy for PD0325901 is 50.0%\n",
      "\n",
      "Running the models with a 5-fold cross-validation...\n",
      "Please wait. This can take up to a minute.\n",
      "All the models were run on PD0325901 successfully!\n",
      "_________________________________________________________________________________________________________\n",
      "Investigating Refametinib... \n",
      "Number of cells:  813\n",
      "\n",
      "Categorisation of the data...\n",
      "\tStructure of the final matrix for Refametinib:\n",
      "\t\tNumber of resistant cells:  203\n",
      "\t\tNumber of sensitive cells:  203\n",
      "\t\tTotal number of cells:  406\n",
      "\n",
      "\t\tThe baseline accuracy for Refametinib is 50.0%\n",
      "\n",
      "Running the models with a 5-fold cross-validation...\n",
      "Please wait. This can take up to a minute.\n",
      "All the models were run on Refametinib successfully!\n",
      "_________________________________________________________________________________________________________\n",
      "Investigating VX-11e... \n",
      "Number of cells:  831\n",
      "\n",
      "Categorisation of the data...\n",
      "\tStructure of the final matrix for VX-11e:\n",
      "\t\tNumber of resistant cells:  208\n",
      "\t\tNumber of sensitive cells:  207\n",
      "\t\tTotal number of cells:  415\n",
      "\n",
      "\t\tThe baseline accuracy for VX-11e is 50.0%\n",
      "\n",
      "Running the models with a 5-fold cross-validation...\n",
      "Please wait. This can take up to a minute.\n",
      "All the models were run on VX-11e successfully!\n",
      "_________________________________________________________________________________________________________\n",
      "Investigating Afatinib... \n",
      "Number of cells:  840\n",
      "\n",
      "Categorisation of the data...\n",
      "\tStructure of the final matrix for Afatinib:\n",
      "\t\tNumber of resistant cells:  210\n",
      "\t\tNumber of sensitive cells:  209\n",
      "\t\tTotal number of cells:  419\n",
      "\n",
      "\t\tThe baseline accuracy for Afatinib is 50.0%\n",
      "\n",
      "Running the models with a 5-fold cross-validation...\n",
      "Please wait. This can take up to a minute.\n",
      "All the models were run on Afatinib successfully!\n",
      "_________________________________________________________________________________________________________\n",
      "Investigating Pelitinib... \n",
      "Number of cells:  832\n",
      "\n",
      "Categorisation of the data...\n",
      "\tStructure of the final matrix for Pelitinib:\n",
      "\t\tNumber of resistant cells:  208\n",
      "\t\tNumber of sensitive cells:  207\n",
      "\t\tTotal number of cells:  415\n",
      "\n",
      "\t\tThe baseline accuracy for Pelitinib is 50.0%\n",
      "\n",
      "Running the models with a 5-fold cross-validation...\n",
      "Please wait. This can take up to a minute.\n",
      "All the models were run on Pelitinib successfully!\n",
      "_________________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>SVM</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>MLP</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drug</th>\n",
       "      <th>Methods</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">CI-1040</th>\n",
       "      <th>Principal Component Analysis</th>\n",
       "      <td>76.3158</td>\n",
       "      <td>76.5789</td>\n",
       "      <td>76.8421</td>\n",
       "      <td>76.3158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recursive Feature Elimination</th>\n",
       "      <td>76.8421</td>\n",
       "      <td>76.8421</td>\n",
       "      <td>76.8421</td>\n",
       "      <td>76.3158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Feature Selection</th>\n",
       "      <td>76.3158</td>\n",
       "      <td>76.5789</td>\n",
       "      <td>76.5789</td>\n",
       "      <td>76.3158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Feature Selection</th>\n",
       "      <td>76.3158</td>\n",
       "      <td>76.5789</td>\n",
       "      <td>76.5789</td>\n",
       "      <td>76.3158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No feature selection</th>\n",
       "      <td>76.3158</td>\n",
       "      <td>76.5789</td>\n",
       "      <td>76.5789</td>\n",
       "      <td>76.3158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">PD0325901</th>\n",
       "      <th>Principal Component Analysis</th>\n",
       "      <td>72.8947</td>\n",
       "      <td>73.6842</td>\n",
       "      <td>73.4211</td>\n",
       "      <td>72.8947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recursive Feature Elimination</th>\n",
       "      <td>72.6316</td>\n",
       "      <td>72.6316</td>\n",
       "      <td>72.6316</td>\n",
       "      <td>72.6316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Feature Selection</th>\n",
       "      <td>73.1579</td>\n",
       "      <td>73.6842</td>\n",
       "      <td>73.4211</td>\n",
       "      <td>72.6316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Feature Selection</th>\n",
       "      <td>73.1579</td>\n",
       "      <td>73.6842</td>\n",
       "      <td>73.4211</td>\n",
       "      <td>72.6316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No feature selection</th>\n",
       "      <td>73.1579</td>\n",
       "      <td>73.6842</td>\n",
       "      <td>73.4211</td>\n",
       "      <td>72.6316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Refametinib</th>\n",
       "      <th>Principal Component Analysis</th>\n",
       "      <td>75.8537</td>\n",
       "      <td>76.3415</td>\n",
       "      <td>75.3659</td>\n",
       "      <td>75.3659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recursive Feature Elimination</th>\n",
       "      <td>73.4146</td>\n",
       "      <td>73.6585</td>\n",
       "      <td>73.6585</td>\n",
       "      <td>73.4146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Feature Selection</th>\n",
       "      <td>75.8537</td>\n",
       "      <td>76.3415</td>\n",
       "      <td>75.3659</td>\n",
       "      <td>74.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Feature Selection</th>\n",
       "      <td>75.8537</td>\n",
       "      <td>76.3415</td>\n",
       "      <td>75.3659</td>\n",
       "      <td>74.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No feature selection</th>\n",
       "      <td>75.8537</td>\n",
       "      <td>76.3415</td>\n",
       "      <td>75.3659</td>\n",
       "      <td>74.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">VX-11e</th>\n",
       "      <th>Principal Component Analysis</th>\n",
       "      <td>69.8795</td>\n",
       "      <td>72.2892</td>\n",
       "      <td>70.8434</td>\n",
       "      <td>70.8434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recursive Feature Elimination</th>\n",
       "      <td>70.1205</td>\n",
       "      <td>69.8795</td>\n",
       "      <td>69.8795</td>\n",
       "      <td>69.8795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Feature Selection</th>\n",
       "      <td>69.8795</td>\n",
       "      <td>72.2892</td>\n",
       "      <td>71.0843</td>\n",
       "      <td>71.5663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Feature Selection</th>\n",
       "      <td>69.8795</td>\n",
       "      <td>72.2892</td>\n",
       "      <td>71.0843</td>\n",
       "      <td>71.5663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No feature selection</th>\n",
       "      <td>69.8795</td>\n",
       "      <td>72.2892</td>\n",
       "      <td>71.0843</td>\n",
       "      <td>71.5663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Afatinib</th>\n",
       "      <th>Principal Component Analysis</th>\n",
       "      <td>60.4762</td>\n",
       "      <td>60.7143</td>\n",
       "      <td>60.7143</td>\n",
       "      <td>60.4762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recursive Feature Elimination</th>\n",
       "      <td>53.0952</td>\n",
       "      <td>53.0952</td>\n",
       "      <td>53.0952</td>\n",
       "      <td>53.0952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Feature Selection</th>\n",
       "      <td>60.4762</td>\n",
       "      <td>60.7143</td>\n",
       "      <td>60.9524</td>\n",
       "      <td>60.9524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Feature Selection</th>\n",
       "      <td>60.4762</td>\n",
       "      <td>60.7143</td>\n",
       "      <td>60.9524</td>\n",
       "      <td>60.9524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No feature selection</th>\n",
       "      <td>60.4762</td>\n",
       "      <td>60.7143</td>\n",
       "      <td>60.9524</td>\n",
       "      <td>60.9524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Pelitinib</th>\n",
       "      <th>Principal Component Analysis</th>\n",
       "      <td>58.0723</td>\n",
       "      <td>52.7711</td>\n",
       "      <td>55.1807</td>\n",
       "      <td>60.9639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recursive Feature Elimination</th>\n",
       "      <td>53.012</td>\n",
       "      <td>52.2892</td>\n",
       "      <td>53.012</td>\n",
       "      <td>52.5301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Feature Selection</th>\n",
       "      <td>58.0723</td>\n",
       "      <td>52.7711</td>\n",
       "      <td>54.9398</td>\n",
       "      <td>54.4578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Feature Selection</th>\n",
       "      <td>58.0723</td>\n",
       "      <td>52.7711</td>\n",
       "      <td>54.9398</td>\n",
       "      <td>54.4578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No feature selection</th>\n",
       "      <td>58.0723</td>\n",
       "      <td>52.7711</td>\n",
       "      <td>54.9398</td>\n",
       "      <td>54.4578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 SVM Logistic Regression  \\\n",
       "Drug        Methods                                                        \n",
       "CI-1040     Principal Component Analysis     76.3158             76.5789   \n",
       "            Recursive Feature Elimination    76.8421             76.8421   \n",
       "            Lasso Feature Selection          76.3158             76.5789   \n",
       "            Random Forest Feature Selection  76.3158             76.5789   \n",
       "            No feature selection             76.3158             76.5789   \n",
       "PD0325901   Principal Component Analysis     72.8947             73.6842   \n",
       "            Recursive Feature Elimination    72.6316             72.6316   \n",
       "            Lasso Feature Selection          73.1579             73.6842   \n",
       "            Random Forest Feature Selection  73.1579             73.6842   \n",
       "            No feature selection             73.1579             73.6842   \n",
       "Refametinib Principal Component Analysis     75.8537             76.3415   \n",
       "            Recursive Feature Elimination    73.4146             73.6585   \n",
       "            Lasso Feature Selection          75.8537             76.3415   \n",
       "            Random Forest Feature Selection  75.8537             76.3415   \n",
       "            No feature selection             75.8537             76.3415   \n",
       "VX-11e      Principal Component Analysis     69.8795             72.2892   \n",
       "            Recursive Feature Elimination    70.1205             69.8795   \n",
       "            Lasso Feature Selection          69.8795             72.2892   \n",
       "            Random Forest Feature Selection  69.8795             72.2892   \n",
       "            No feature selection             69.8795             72.2892   \n",
       "Afatinib    Principal Component Analysis     60.4762             60.7143   \n",
       "            Recursive Feature Elimination    53.0952             53.0952   \n",
       "            Lasso Feature Selection          60.4762             60.7143   \n",
       "            Random Forest Feature Selection  60.4762             60.7143   \n",
       "            No feature selection             60.4762             60.7143   \n",
       "Pelitinib   Principal Component Analysis     58.0723             52.7711   \n",
       "            Recursive Feature Elimination     53.012             52.2892   \n",
       "            Lasso Feature Selection          58.0723             52.7711   \n",
       "            Random Forest Feature Selection  58.0723             52.7711   \n",
       "            No feature selection             58.0723             52.7711   \n",
       "\n",
       "                                                 MLP Random Forest  \n",
       "Drug        Methods                                                 \n",
       "CI-1040     Principal Component Analysis     76.8421       76.3158  \n",
       "            Recursive Feature Elimination    76.8421       76.3158  \n",
       "            Lasso Feature Selection          76.5789       76.3158  \n",
       "            Random Forest Feature Selection  76.5789       76.3158  \n",
       "            No feature selection             76.5789       76.3158  \n",
       "PD0325901   Principal Component Analysis     73.4211       72.8947  \n",
       "            Recursive Feature Elimination    72.6316       72.6316  \n",
       "            Lasso Feature Selection          73.4211       72.6316  \n",
       "            Random Forest Feature Selection  73.4211       72.6316  \n",
       "            No feature selection             73.4211       72.6316  \n",
       "Refametinib Principal Component Analysis     75.3659       75.3659  \n",
       "            Recursive Feature Elimination    73.6585       73.4146  \n",
       "            Lasso Feature Selection          75.3659        74.878  \n",
       "            Random Forest Feature Selection  75.3659        74.878  \n",
       "            No feature selection             75.3659        74.878  \n",
       "VX-11e      Principal Component Analysis     70.8434       70.8434  \n",
       "            Recursive Feature Elimination    69.8795       69.8795  \n",
       "            Lasso Feature Selection          71.0843       71.5663  \n",
       "            Random Forest Feature Selection  71.0843       71.5663  \n",
       "            No feature selection             71.0843       71.5663  \n",
       "Afatinib    Principal Component Analysis     60.7143       60.4762  \n",
       "            Recursive Feature Elimination    53.0952       53.0952  \n",
       "            Lasso Feature Selection          60.9524       60.9524  \n",
       "            Random Forest Feature Selection  60.9524       60.9524  \n",
       "            No feature selection             60.9524       60.9524  \n",
       "Pelitinib   Principal Component Analysis     55.1807       60.9639  \n",
       "            Recursive Feature Elimination     53.012       52.5301  \n",
       "            Lasso Feature Selection          54.9398       54.4578  \n",
       "            Random Forest Feature Selection  54.9398       54.4578  \n",
       "            No feature selection             54.9398       54.4578  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_all_drugs(DRUG_NAMES, PATH, SCORE, N_SPLITS, verbose = 5):\n",
    "    \n",
    "    final_results = pd.DataFrame()\n",
    "    \n",
    "    for drug in DRUG_NAMES:\n",
    "\n",
    "        print(f\"Investigating {drug}... \")\n",
    "\n",
    "        # Read the corresponding ML matrix\n",
    "        ML_matrix = pd.read_csv(PATH + f\"{drug}.csv\")\n",
    "        ML_matrix.set_index(\"Cell_line\", inplace = True)\n",
    "\n",
    "        print(\"Number of cells: \", len(ML_matrix.index))\n",
    "        if verbose > 3:\n",
    "            ML_matrix.head() #Show the head of the dataframe for that drug\n",
    "\n",
    "        results_drug = calculate_performances(drug, ML_matrix, PATH, SCORE, N_SPLITS, verbose = verbose)\n",
    "        \n",
    "        final_results = pd.concat([final_results,results_drug])\n",
    "                                          \n",
    "        print(\"_________________________________________________________________________________________________________\")\n",
    "        \n",
    "    final_results.index.name = \"Methods\"\n",
    "    \n",
    "    return final_results.reset_index(drop=False).set_index([\"Drug\",\"Methods\"])\n",
    "                                          \n",
    "final_results = run_all_drugs(DRUG_NAMES, PATH, SCORE, N_SPLITS, verbose = 0)\n",
    "display(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with the code -- Not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRUG_NAME = \"CI-1040\"\n",
    "\n",
    "# ML_matrix = pd.read_csv(PATH + f\"CI-1040.csv\")\n",
    "# ML_matrix.set_index(\"Cell_line\", inplace = True)\n",
    "\n",
    "# ML_matrix[\"Response\"] = pd.cut(ML_matrix[DRUG_NAME], [np.min(ML_matrix[DRUG_NAME]), ML_matrix[DRUG_NAME].quantile([0.25,0.75])[0.25], ML_matrix[DRUG_NAME].quantile([0.25,0.75])[0.75], np.max(ML_matrix[DRUG_NAME])], labels = [\"sensitive\",\"medium\", \"resistant\"])\n",
    "# ML_matrix = ML_matrix.drop([DRUG_NAME], axis = 1)\n",
    "    \n",
    "# #Drop all the \"medium\" classes and NaNs\n",
    "# ML_matrix = ML_matrix.drop(ML_matrix[ML_matrix[\"Response\"] == \"medium\"].index)\n",
    "# ML_matrix = ML_matrix.dropna()  \n",
    "\n",
    "# # Convert the response to a float\n",
    "# ML_matrix[\"Response\"] = [0 if x=='resistant' else 1 for x in ML_matrix['Response']]\n",
    "\n",
    "# X = ML_matrix.drop(\"Response\", axis = 1)\n",
    "# #X = X[X.columns[[False,False,False,True,True,True]]]\n",
    "# y = ML_matrix[\"Response\"].astype('float64')\n",
    "        \n",
    "#     # Define our models    \n",
    "# svm = SVC(random_state=40)\n",
    "# lr = LogisticRegression(solver= 'saga', max_iter=5000, random_state=41)\n",
    "# mlp = MLPClassifier(max_iter=5000, random_state=42)\n",
    "# rf = RandomForestClassifier(n_estimators=300, max_depth = 3, max_leaf_nodes = 10, random_state=43)\n",
    "\n",
    "# svm_scores = cross_val_score(svm,X,y,cv=5)  # fit + predict + eval. \n",
    "# print('SVM Average Accuracy:', str(round(svm_scores.mean()*100,3)),'%')\n",
    "# lr_scores = cross_val_score(lr,X,y,cv=5)  # fit + predict + eval. \n",
    "# print('LR Average Accuracy:', str(round(lr_scores.mean()*100,3)),'%')\n",
    "# mlp_scores = cross_val_score(mlp,X,y,cv=5)  # fit + predict + eval. \n",
    "# print('Perceptron Average Accuracy:', str(round(mlp_scores.mean()*100,3)),'%')\n",
    "# rf_scores = cross_val_score(rf,X,y,cv=5)  # fit + predict + eval. \n",
    "# print('RF Average Accuracy:', str(round(rf_scores.mean()*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - None of the methods of feature selection have lead to satisfying results. Alternative solutions have to be investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The GDSC portal provides a list of 659 features that are frequently altered in cancer and are correlated with drug sensitivity. We will use these to add different weights to features.\n",
    "- We will experiment with Pearson's coefficient, Spearman's correlation and Elastic Net in feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
